---
title: "R Notebook"
output: html_notebook
---

Source: http://mxnet.incubator.apache.org/tutorials/r/mnistCompetition.html

Now let’s use a new network structure: LeNet. It has been proposed by Yann LeCun for recognizing handwritten digits. We’ll demonstrate how to construct and train a LeNet in mxnet.

```{r}
require(mxnet)

    train <- read.csv('train.csv', header=TRUE)
    test <- read.csv('test.csv', header=TRUE)
    train <- data.matrix(train)
    test <- data.matrix(test)

    train.x <- train[,-1]
    train.y <- train[,1]
    
    train.x <- t(train.x/255)
    test <- t(test/255)
    
     table(train.y)
```

```{r}
# construct the network

# input
data <- mx.symbol.Variable('data')
# first conv
conv1 <- mx.symbol.Convolution(data=data, kernel=c(5,5), num_filter=20)
tanh1 <- mx.symbol.Activation(data=conv1, act_type="tanh")
pool1 <- mx.symbol.Pooling(data=tanh1, pool_type="max",
                          kernel=c(2,2), stride=c(2,2))
# second conv
conv2 <- mx.symbol.Convolution(data=pool1, kernel=c(5,5), num_filter=50)
tanh2 <- mx.symbol.Activation(data=conv2, act_type="tanh")
pool2 <- mx.symbol.Pooling(data=tanh2, pool_type="max",
                          kernel=c(2,2), stride=c(2,2))
# first fullc
flatten <- mx.symbol.Flatten(data=pool2)
fc1 <- mx.symbol.FullyConnected(data=flatten, num_hidden=500)
tanh3 <- mx.symbol.Activation(data=fc1, act_type="tanh")
# second fullc
fc2 <- mx.symbol.FullyConnected(data=tanh3, num_hidden=10)
# loss
lenet <- mx.symbol.SoftmaxOutput(data=fc2)
```

```{r}
# reshape the matrices into arrays:
train.array <- train.x
dim(train.array) <- c(28, 28, 1, ncol(train.x))
test.array <- test
dim(test.array) <- c(28, 28, 1, ncol(test))
```


```{r}
# We want to compare training speed on different devices, so define the devices:
n.gpu <- 1
device.cpu <- mx.cpu()
device.gpu <- lapply(0:(n.gpu-1), function(i) {
  mx.gpu(i)
})
# We can pass a list of devices to ask MXNet to train on multiple GPUs (you can do this for CPUs, but because internal computation of CPUs is already multi-threaded, there is less gain than with using GPUs).
```

```{r}
# Start by training on the CPU first. Because this takes a bit time, we run it for just one iteration.
mx.set.seed(0)
tic <- proc.time()
model <- mx.model.FeedForward.create(lenet, X=train.array, y=train.y,
                                     ctx=device.cpu, num.round=1,
                                     array.batch.size=100,
                                     learning.rate=0.05, momentum=0.9,
                                     wd=0.00001,
                                     eval.metric=mx.metric.accuracy,
                                     epoch.end.callback=mx.callback.log.train.metric(100))
```

```{r}
print(proc.time() - tic)
```


```{r}
# train on a GPU
    mx.set.seed(0)
    tic <- proc.time()
model <- mx.model.FeedForward.create(lenet, X=train.array, y=train.y,
                                     ctx=device.gpu, num.round=5,
                                     array.batch.size=100,
                                     learning.rate=0.05, momentum=0.9,
                                     wd=0.00001,
                                     eval.metric=mx.metric.accuracy, 
                                     epoch.end.callback = mx.callback.log.train.metric(100))

    print(proc.time() - tic)    
```

By using a GPU processor, we significantly speed up training! Now, we can submit the result to Kaggle to see the improvement of our ranking!


```{r}
 preds <- predict(model, test.array)
    pred.label <- max.col(t(preds)) - 1
    submission <- data.frame(ImageId=1:ncol(test), Label=pred.label)
    write.csv(submission, file='submission.csv', row.names=FALSE, quote=FALSE)
```

